# -*- coding: utf-8 -*-
"""Strategic Subscriber Segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ABNRVCxCwCF48JSCkEygGRNHbQ5Of11R

# **The Subscriber Segmenteer**
by [Andrew Yu](https://www.linkedin.com/in/andrewdyu/), [Norman Chen](https://www.linkedin.com/in/naiming-chen/), and [Cathy Chen](https://www.linkedin.com/in/cathy-chen-8b5480195/).

---

Check it out on [GitHub](https://github.com/YuDrew/strategic-subscriber-segmentation/).

# **Executive Summary**
---

## **Background & Objective**

In the space of content creation, subscribers rule above all else. With the release of new publishing platforms like Anchor (podcasts) and Substack (newsletters), a new wave of content creators is starting to emerge. As content niches mature, content creators will need to more heavily compete for advertisement deals and sponsorships, and in order to do so, it is imperative that they understand who their subscribers are. However, these new publishing platforms aren’t equipped with the right tools to help creators properly break down their audience by increasingly niche segmentation criteria. This makes it hard for creators to curate good content for their audience, and it’s doubly difficult to secure advertisement deals if they can only guess at their audience base. 

We hope to identify a scalable model that can quickly and accurately categorize subscribers of entrepreneurship-focused content into useful categories (e.g.  investors vs startup operators vs students). Ideally, this tool should be:

1. cost-free considering that many creators are not yet monetized
2. relatively accurate and granular, and
3. repeatable and easy to run OR continuously and automatically updated.

## **Data**

To build this model, we used a redacted subscriber snapshot of [Climate Tech VC](https://climatetechvc.substack.com/), a rapidly-growing Substack newsletter focused on climate innovation. The starting dataset consists 8662 website URLs and can be found [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/subscriber_websites.csv).

In order to transform this data into meaningful information,we pulled the text from the homepage of each website. The results of the initial scrape and cleaning can be found [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/scraped_website_text.csv).

After processing this scraped data, we end with a dataset of all subscribers with English websites and the plaintext from their website's homepage, which can be found [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/english_cleaned_all.csv).

## **Findings**

For our applications, K-means clustering actually offers a very powerful tool for subscriber segmentation. A simple Term Frequency, Inverse Document Frequency (TF-IDF) transformation provides ample information to meaningfully cluster our documents. 

The LDA topic modeling was able to provide similarly salient segments, but because LDA is a more complex model than K-Means, we find that the simpler solution is likely the better solution.

---

# Part 0: Notebook Setup
---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --upgrade gspread
# !pip install flashtext
# !pip install lxml
# !pip install beautifulsoup4
# !pip install htmlmetadata
# !pip install -U gensim
# !pip install pyldavis==2.1.2
# !pip install requests_html

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Important Data Libs
# import pandas as pd
# import numpy as np
# 
# import os
# 
# # Important Viz Libs
# import seaborn as sns
# import matplotlib.pyplot as plt
# import plotly
# import plotly.graph_objects as go
# import plotly.offline as offline
# from plotly.graph_objs import *
# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
# 
# # Text Processsing
# import re
# 
# # LDA Topic Modeling
# from gensim import corpora
# from gensim import matutils
# from gensim.models import LsiModel, KeyedVectors, LdaModel
# from gensim.models.tfidfmodel import TfidfModel
# from gensim.models.nmf import Nmf
# from gensim.models import HdpModel
# 
# # LDA Visualization
# import pyLDAvis
# 
# # Important Export Libs
# import gspread

# Important Data Visualization Imports
import matplotlib.pyplot as plt
import seaborn as sns

# Let's make our data vis look better
plt.rc('figure',figsize=(10,7)) # Make our plots nice and big
plt.rc("font", size=14) # Make our fonts nice and big too
sns.set_style('darkgrid') # who doesn't like a dark theme
sns.set_color_codes('dark') # once again, dark themes rule

"""# Part 1: Initial Data, Processing, & EDA
---

## 1.1 Starting with Websites

As previously mentioned, content creators often only have the list of emails for their audience (e.g. through their mailing list), plus a few additional fields that they may or may not have included in their audience onboarding process. 

For this exploration, we'll use a snapshot of the subscriber base of a Substack newsletter focused on climate tech. We've already exported this data, so we'll need to load it in via Google Drive.
"""

websites_df = pd.read_csv('https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/subscriber_websites.csv')
websites_df

"""Taking a look at our data, it's clear that there are two big issues: 
1. Domain names alone isn't very much data to analyze, and segmentation would likely be very difficult. 
2. There are several "repeats" of websites in the data.
To see the extent of these repeats, let's take a look at the frequencies of these websites.
"""

website_frequencies = websites_df['Website'].value_counts()
website_frequencies

"""We have a lot of repeats in this subscriber list! We don't want to have to parse each of these multiple times, so let's get unique ones, only."""

unique_sites_df = pd.DataFrame(websites_df.Website.unique(), columns=['url'])
unique_sites_df

"""## 1.2 Getting More Data

In order to make meaningful customer clusters, we need more data. The only data we have access to is this list of websites - from here we can take only a handful of approaches.
1. If we know there's a comprehensive database of websites and classficiations, we could directly query that database. 
2. If we can't find or access such a database, we can instead go directly to the website and extract more information from each website. 

Because we don't have access to Option 1, we instead chose Option 2: scraping text from all the websites.

**Note**: Because scraping takes such a long time, especially with larger datasets, we've omitted the scraping from this notebook and instead have exported the results of our initial scrape and preprocessing [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/scraped_website_text.csv). We've instead opted to outline our process for the scrape in text.

### 1.2.1 Initial Scrape

In order to create meaningful data out of website URLs, we scraped plaintext from each website using the requests package, and we cleaned the data using the BeautifulSoup4 package. To speed up processes, we multithreaded our scrape.

```python
import requests
from bs4 import BeautifulSoup
import htmlmetadata
from htmlmetadata import extract_metadata
from lxml import html
from requests_html import HTMLSession
import concurrent.futures
import time

headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
MAX_THREADS = 1000
```

The first function makes helps us get the strings from an HTML document.

```python
# Clean up Data with BeautifulSoup4
# @param html_code is a byte-type html document, generated typically by a requests.content call.
# @returns a list of "stripped strings"; BeautifulSoup4 separates out HTML code into different tags, and the text is extracted from each tag.
def get_strips(html_code):
  soup = BeautifulSoup(html_code, 'lxml')

  for script in soup(['script','style']):
      script.decompose()
        
  strips = list(soup.stripped_strings)
  return strips

```

The second function calls the actual request to get data from the website.

```python
# Scrape data from a given site. 
# @param site is the URL of the website to be scraped, ind is the index of the site in relation to the dataframe that this function is called on
# @return a whole host of data scraped from the website, including error logging, metadata description, metadata language category, and the "strips." 
def scrape_site(site, ind):
  print(site)
  
  # Generate potential URLs to loop through
  url_starts = ['https://', 'http://', 'https://www.', 'http://www.']
  urls = [start + site for start in url_starts]

  # Define Return Variables
  meta_description = ''
  meta_language = ''
  html_code = ''
  strips = []
  errors = []

  for url in urls:
    try:
      page = requests.get(url, timeout = 5, headers = headers)
      meta = extract_metadata(url)
      meta_description = meta['summary'].get('description')
      meta_language = meta['summary'].get('language')
      html_code = page.content
      strips = get_strips(html_code)
      break
    except Exception as exc:
      errors.append(exc)
  return meta_description, meta_language, str(html_code), strips, errors, ind

```

And this third function allows us to concurrently run the first two functions on every website in a dataframe.

```python
# Concurrently call scrape_site on all sites across a dataframe
#@param a dataframe that has been properly set up with error, description, language, html_code, and strips columns.
#@return a dataframe populated by all site scrapes.
def scrape_all_sites(df):
  with concurrent.futures.ThreadPoolExecutor(max_workers = MAX_THREADS) as executor:
    future_load_site = {executor.submit(scrape_site, df['url'][ind], ind): ind for ind in df.index}
    for future in concurrent.futures.as_completed(future_load_site):
        url = future_load_site[future]
        try:
          meta_description, meta_language, html_code, strips, errors, ind = future.result()
          df['description'][ind] = meta_description
          df['language'][ind] = meta_language
          df['html_code'] = html_code
          df['strips'][ind] = strips
        except Exception as exc:
          df['error'][ind].append(exc)
  return df
```

### 1.2.2 Initial Preprocessing

Once the data was consolidated into a dataframe, `processing_df`, we did basic regular expression extraction to start the preprocessing.

```python
# Scrape the Sites
processing_df = scrape_all_sites(websites_df)

# Consolidate Strips into Text and Remove Non-Words
processing_df['text'] = processing_df['strips'].apply(lambda x: re.sub(r"@[^\w\s]", '',' '.join(x)))

# Lowercase Everything
processing_df['text']= processing_df['text'].map(lambda x: x.lower())
```

And then we exported the dataframe using the Pandas `.to_csv()` function. You can find the exported data [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/scraped_website_text.csv).

## 1.3 Processing the Data

After scraping the data, we need to process it in several different ways:
1. **Additional Cleanup** (removing punctuation, leftover HTML code, etc.)
2. **Language Detection** (removing websites that aren't in English, since the packages we'll be using are focused on the English language)
3. **Lemmatization** (oftentimes, we'll have similar words that share roots or are variations on the same word. We want to combine those all together)
4. **Remove Stopwords** (removing words that are too common and might skew our analyses)

We'll start with `scraped_text_df`, a dataframe that resulted from concurrently querying every website for their metadata description and language tag (using the [htmlmetadata](https://pypi.org/project/htmlmetadata/) package) and raw text from each website (using the [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/) package).
"""

scraped_text_df = pd.read_csv('https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/scraped_website_text.csv')
scraped_text_df

"""### 1.3.1 Additional Cleanup

Because we just loaded this dataset in from Github, the detault datatype for every column is "object."
"""

scraped_text_df.dtypes

"""We need to ensure that all of our columns are of type string. However, in Python strings are default stored as Object datatypes due to their variable length. In order to conserve memory, the dtypes won't change at face value, but when the data is processed and certain methods are called they'll be properly inferred to be strings. """

new_dtypes = {"url": str, "error": str, "meta": str, "language":str, "text":str}

scraped_text_df = scraped_text_df.astype(new_dtypes)
scraped_text_df.dtypes

"""Next, let's go ahead and combine `meta` and `text`, as long as one of the two exist."""

def combine_meta_and_text(meta, text):
  if meta.lower() == 'nan' and text.lower() == 'nan':
     return ''
  elif meta.lower() == 'nan':
    return text
  elif text.lower() == 'nan':
    return meta
  else:
    return meta.lower() + ' ' + text.lower()

scraped_text_df['text'] = scraped_text_df[['meta','text']].apply(lambda x: combine_meta_and_text(x['meta'], x['text']), axis=1)

"""There's still a little bit of additional HTML, let's remove that with the html2text package. 

We'll also get rid of additional punctuation using the string.translate function. 
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install html2text
# import html2text
# import string

h = html2text.HTML2Text()
scraped_text_df['text'] = scraped_text_df['text'].apply(lambda x: h.handle(x).translate(str.maketrans('','', string.punctuation)))

scraped_text_df['text'].head()

"""### 1.3.2 Language Classfication

Furthermore, you'll have noticed that some of the websites aren't in English. While it would be fascinating to explore websites in other languages, it makes clustering a little difficult, so we'll drop all those websites that aren't in English.

To do so, we can rely on both the metadata descriptions for languages, and in the absence of those descriptions, we can instead use the `spacy` `LanguageDetector` to determine whether or not something is in English.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install spacy_langdetect
# !pip install spacy
# from spacy_langdetect import LanguageDetector
# import spacy

nlp = spacy.load('en')
nlp.add_pipe(LanguageDetector(), name='language_detector', last = True)

def check_english(language_tag, text):
  if language_tag == 'en':
    return True
  elif nlp(text)._.language.get('language') == 'en':
    return True
  else:
    return False

scraped_text_df['is_english'] = scraped_text_df.apply(lambda x: check_english(x['language'],x['text']), axis=1)

scraped_text_df.head()

"""Now, we can drop websites that aren't classified as English."""

english_only_df = scraped_text_df[scraped_text_df['is_english']]
english_only_df

"""It's important to note that we just dropped nearly a thousand websites. This newsletter sure has a lot of foreign followers. 

This particular process can takes quite a while, too, so we've exported it [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/english_only.csv).

### 1.3.3 Lemmatization 
Lastly, we want to reduce words to their core meaning and lumping together words that essentially mean the same thing (i.e. companies vs company). There are two appraoches we could take here: (1) stemming (removing suffixes in order to group words by root, resulting in word stems like `compan` for `companies`, `company` and even `companion`) and (2) lemmatization (reducing words to a standard lemma, bringing `companies` and `company` both to `company` and leaving `companion` alone). 

In order to make for a user-friendly experience, we prefer lemmatization.

We'll use the NLTK WordNetLemmatizer in order to lemmatize our data.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import nltk
# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger')
# from nltk.stem import WordNetLemmatizer
# from nltk.corpus import wordnet

def get_wordnet_pos(word):
  tag = nltk.pos_tag([word])[0][1][0].upper()
  tag_dict = {'J': wordnet.ADJ,
              'N': wordnet.NOUN,
              'V': wordnet.VERB,
              'R': wordnet.ADV}
  return tag_dict.get(tag, wordnet.NOUN)

def tokenize_and_lemmatize(document):
  lemmatizer = WordNetLemmatizer()
  word_list = nltk.word_tokenize(document)
  lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list])
  return lemmatized_output

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# english_only_df['text'] = english_only_df['text'].apply(lambda x: tokenize_and_lemmatize(x))

english_only_df

"""### 1.3.4 Removing Stopwords

Now that we've got cleaned, lemmatized, English-only documents, it's time to wrap up our processing by ripping out all the stopwords that will clutter up our data. 

We'll use gensim simple_preprocess, which lowercases, tokenizes, and de-accents words and the NLTK stopwords list to remove those stopwords from the tokenized list.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import gensim
# from gensim.utils import simple_preprocess
# import nltk
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# 
# stop_words = stopwords.words('english')
# stop_words.extend(['showicon', 'datatype'])
# 
# def remove_stopwords_single_text(doc):
#   return [word for word in simple_preprocess(str(doc)) if word not in stop_words]
#

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# english_only_df['text_cleaned'] = english_only_df['text'].apply(lambda x: remove_stopwords_single_text(x))
# english_only_df['text_cleaned_string'] = english_only_df['text_cleaned'].apply(lambda x: ' '.join(x))

english_only_df

"""## 1.4 Verifying Our Preprocessing

Now that we've done so much preprocessing, let's take a look at our data.
"""

from wordcloud import WordCloud

long_string = ','.join(list(english_only_df['text_cleaned_string'].values))

wordcloud = WordCloud(background_color='black', \
                      max_words = 100, \
                      contour_width = 3, \
                      contour_color = 'steelblue', \
                      width = 800, height = 400).generate(long_string)

wordcloud.to_image()

"""At this point, we can get rid of all the extra fields and come back to just our new, processed text. We also exported this as a CSV using the Pandas `to_csv()` function, which you can access [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-segmentation/main/english_cleaned_unique.csv)."""

english_cleaned_unique_df = english_only_df[['url','text_cleaned_string']].rename(columns={'text_cleaned_string':'text'})
english_cleaned_unique_df

english_cleaned_unique_df.to_csv('english_cleaned_unique.csv')

"""However, there is one final step. We don't want websites that have a single instance overpowering websites that have several subscribers, so we re-join our dataset with our initial website list in order to account for frequency. Again, we exported this as a CSV using the Pandas `to_csv()` function, which you can access [here](https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/english_cleaned_all.csv)."""

english_cleaned_all_df = websites_df.merge(english_cleaned_unique_df, left_on='Website', right_on='url', how= 'inner')
english_cleaned_all_df = english_cleaned_all_df[['url','text']]
english_cleaned_all_df

english_cleaned_all_df.to_csv('english_cleaned_all.csv')

"""Interestingly, the word frequencies change drastically after we re-weight / allow for duplicate websites. Microsoft Outlook makes a heavy appearance here."""

from wordcloud import WordCloud

long_string = ','.join(list(english_cleaned_all_df['text'].values))

wordcloud = WordCloud(background_color='black', \
                      max_words = 100, \
                      contour_width = 3, \
                      contour_color = 'steelblue', \
                      width = 800, height = 400).generate(long_string)

wordcloud.to_image()

"""# Part 2: Clustering
---
With our new cleaned documents, we begin segmenting our readers with various methods. We first try K-Means clustering, and then we try Topic Modeling with Latent Dirchlet Allocation.

## 2.1 K-Means Clustering

The first thing that comes to mind when we consider clustering is K-Means clustering. Unfortunately, we can't directly compute Euclidean distance between words, so we need to conduct some feature engineering in order to conduct K-Means clustering.

We opted to use `TF-IDF` (or Term Frequency-Inverse Document Frequency) to compute the importance of a word in a corpus, with the most relevant terms being the most important. The more times a word appears within a document, the stronger that word is (hence Term Frequency), but the more documents the word appears in, the less unique / salient that word is (hence Inverse Document Frequency).

### 2.1.1 Selecting the Right Data
Because of the way Inverse Document Frequency works, it's best for us to use TF-IDF on our `english_cleaned_unique_df` dataframe, rather than dataframes that have high repeats. Otherwise, all words associated with Cornell University might be unfairly underweighted. Sorry, Cornell.
"""

english_cleaned_unique_df

"""### 2.1.2 Computing TF-IDF

To compute TF-IDF, we use the sklearn TfidfVectorizer.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
corpus = english_cleaned_unique_df['text']
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())
tf_idf.T.nlargest(5, 0)

tf_idf

"""Note that in the end, we want to categorize each website with a cluster, so we will run k-means on tf_idf instead of its transpose. In the table above, each entry represents the tf-idf value of the word (the column) in the particular website (the row). 

**Note on TF and IDF:**
- Since idf value punishes the number of different websites a word has appeared in, the idf value for each word is the same across all websites. 
- Since the tf (term frequency) of each word is calculated by website and is thus different across websites. This is why each word will still have a different tf-idf value in each website.

### 2.1.3 Conducting K-Means

Now that we have the tf-idf values, we can calculate distance. This means we can categorize each website by the combination of the tf-idf values of every words.

The first step of k-means is to determine our `k`. To do this, we use the "elbow method" to see the change in the sum of squared distances as we increase the number of clusters.
"""

from sklearn.cluster import KMeans
Sum_of_squared_distances = []
K = range(3,15)
for k in K:
    km = KMeans(n_clusters=k, random_state=1, max_iter = 50)
    km = km.fit(tf_idf)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""We can see that the dip is around when k=13. We will use this value for our number of clusters and train our final k-means model, `km`."""

km = KMeans(n_clusters=13, random_state=1)
km = km.fit(tf_idf)

"""### 2.1.4 Visualizing K-Means

Now that we have our model, we'll want to do the following: 
1. Vizualize our K-Means clusters;
2. Create word clouds for our clusters; and
3. Try to label our clusters ourselves.

First, let's define some functions to extract the top words of each cluster (`get_top_features_cluster`) and vizualize those words with a barplot (`plotWords`).
"""

def get_top_features_cluster(tf_idf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction==label) # indices for each cluster
        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores
        features = vectorizer.get_feature_names()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['features', 'score'])
        dfs.append(df)
    return dfs

def plotWords(dfs, n_feats):
    plt.figure(figsize=(8, 4))
    for i in range(0, len(dfs)):
        plt.title(("Most Common Words in Cluster {}".format(i)), fontsize=10, fontweight='bold')
        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])
        plt.show()

import warnings
warnings.filterwarnings('ignore')

final_df_array = tf_idf.to_numpy()
prediction = km.predict(final_df_array)
n_feats = 20
dfs = get_top_features_cluster(final_df_array, prediction, n_feats)
plotWords(dfs, 13)

"""We can see some very clear trends for words within given clusters. To further vizualize this we, use word clouds."""

# Transforms a centroids dataframe into a dictionary to be used on a WordCloud.
def centroidsDict(centroids, index):
    a = centroids.T[index].sort_values(ascending = False).reset_index().values
    centroid_dict = dict()

    for i in range(0, len(a)):
        centroid_dict.update( {a[i,0] : a[i,1]} )

    return centroid_dict

def generateWordClouds(centroids):
    wordcloud = WordCloud(max_font_size=100, background_color = 'white')
    for i in range(0, len(centroids)):
        centroid_dict = centroidsDict(centroids, i)        
        wordcloud.generate_from_frequencies(centroid_dict)
        plt.figure()
        plt.title('Cluster {}'.format(i))
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.show()

final_df_array

centroids = pd.DataFrame(km.cluster_centers_)
centroids.columns = tf_idf.columns
generateWordClouds(centroids)

"""Here the patterns emerge. Based off the word clouds, these are our guesses.

| Cluster Number | Our Best Guess |
| --- | --- |
| 0 | Sign-In Text for Personal Emails |
| 1 | Businesses / Startups |
| 2 | Geographic Locations and News |
| 3 | Climate Change (Carbon / Emissions Focus) |
| 4 | Microsoft & Outlook Users |
| 5 | High Finance & Later Stage Finance |
| 6 | Personal Domains / Email Forwarding / Error Messages |
| 7 | Venture Capital |
| 8 | Random Bits of Javascript and HTML | 
| 9 | Students / Academia |
| 10 | Carbon Capture and Emissions |
| 11 | Energy / Renewables |
| 12 | More Bits of Javascript and HTML |

### 2.1.5 Putting K-Means to the Test

Now that we have our K-Means model, we use these trained assignments to assign each of our documents (each website) to one of these clusters.
"""

labels = km.labels_ 
english_cleaned_unique_df['label'] = labels
english_cleaned_unique_df.head()

"""Next, we can map these numeric cluster labels to our "best guesses"."""

label_map = {
 0: 'Sign-In Text for Personal Emails',
 1: 'Businesses / Startups',
 2: 'Geographic Locations and News',
 3: 'Climate Change (Carbon / Emissions Focus)',
 4: 'Microsoft & Outlook Users',
 5: 'High Finance & Later Stage Finance',
 6: 'Personal Domains / Email Forwarding / Error Messages',
 7: 'Venture Capital',
 8: 'Random Bits of Javascript and HTML',
 9: 'Students / Academia',
 10: 'Carbon Capture and Emissions',
 11: 'Energy / Renewables',
 12: 'More Bits of Javascript and HTML'
}
english_cleaned_unique_df['string_label'] = english_cleaned_unique_df['label'].map(label_map)

english_cleaned_unique_df

"""With that completed, we can take a look at the distribution of unique sites by our newly named clusters."""

distribution_df = english_cleaned_unique_df.groupby(by='string_label').count().reset_index()
distribution_df.plot.bar(x='string_label', y='url', title='Size of Each Category (Unique Values Only)', figsize=(20, 10))

"""Lastly, if we want to look at our overall subscriber base, we ought to consider *all* of our subscribers, which includes the repeats."""

labled_english_all_df = english_cleaned_all_df.merge(english_cleaned_unique_df, how='left', on='url')
labled_english_all_df.drop(columns=['text_y'], inplace=True)
labled_english_all_df.rename(columns={'text_x':'text'}, inplace=True)
labled_english_all_df[labled_english_all_df['label']==12]

distribution_df = labled_english_all_df.groupby(by='string_label').count().reset_index()
distribution_df.plot.bar(x='string_label', y='url', title='Size for Each Category (Repeats Included)', figsize=(20, 10))

"""It's interesting to note how much the personal email clusters inflate when we include repeats; this is exactly what we would hope to expect.

We can see that the following clusters dominate:
1. Businesses / Startups (General)
2. Students / Academia
3. Venture Capital
4. Personal Emails & Microsoft and Outlook Users & Sites That Provide Random Javascript

This makes sense considering that the newsletter this dataset is from focuses on Climate Tech, which is the intersection of sustainability and entrepreneurship. However, the overwhelming dominance of Businesses / Startups is interesting. Let's explore this a little deeper.
"""

dominant_df = labled_english_all_df[labled_english_all_df['string_label']=='Businesses / Startups']
dominant_df[['url']].value_counts()

"""This actually makes a lot of sense - generalist businesses, consulting firms, and accelerators all have broader business language in thier website and aren't nearly as targeted as venture capital. Furthermore, we see that is is hyperdispersed (i.e. there's a very loooong tail for this particular category), which is exactly what we'd expect. Nice!

### 2.1.6 Thoughts on K-Means

In general, we can see that our clusters are fairly acurate. In each cluster, we can see that the words are indeed coorelated to each other - suggesting that our metric - tf-idf - is a solid metric to calculate distance for k-means clusters. Using the final distribution of cluster, we can see that Climate Tech VC's subscribers generally align with their goals, focused on climate tech startups, students, and venture capitalists.


Overall, K-Means has a few distinct benefits as well as a few drawbacks. While K-Means clustering is a strong unsupervised learning method, traditional implementations are definitive / exclusive. That is to say, a given word or document is assigned to one cluster and one cluster only. Oftentimes, we'll find words that appear in multiple documents (especially words that might have different definitions depending on context). While K-Means is a strong solution for our particular problem, we explore a probabilistic approach using Latent Dirchlet Allocation.

## 2.2 Topic Modeling with Latent Dirichlet Allocation
---

In order to account for the fact that *not* all words fall nicely into one bucket or the other, and neither do companies, we can try a probabilistic approach to clustering, instead.

We find a potential solution in Latent Dirchlet Allocation (LDA). Without going to into depth (read more [here](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)), the LDA model effectively assumes the following:
1. Length of documents follow some Poisson distribution
2. Documents are a mixture of a fixed number of K "topics" that follows some Dirchelet distribution over those K topics.
3. Words have their own probability of being in each topic (so they can show up in multiple topics).
4. Thus, documents are functionally generated by having some random length, then generating documents by randomly pulling words from topics based off the assumed distribution of topics the document is based on.

### 2.2.1 Data Revisited

For LDA, words can fit in multiple topics, which means that we may want to remove words that appear too much. Instead of computing `TF-IDF` to do this, we instead seek to extract bigrams like "venture capital" and remove terms that might appear everywhere (i.e. company, email, etc.).
"""

# We exported our data before, so let's just read it in that way.
topic_modeling_df = pd.read_csv('https://raw.githubusercontent.com/YuDrew/strategic-subscriber-sorter/main/english_cleaned_unique.csv')
topic_modeling_df = topic_modeling_df[['url','text']].astype({'url':str,'text':str})
topic_modeling_df

#First up, we need to separate the text field into tokens
def doc_to_words(docs):
  for doc in docs:
    yield(gensim.utils.simple_preprocess(str(doc), deacc=True))

docs = topic_modeling_df.text.values.tolist()
docs = list(doc_to_words(docs))

"""We'll create bigrams in order to capture phrases like "venture capital" and "climate tech.""""

from gensim.models import Phrases

# Add bigrams and trigrams to the docs
bigram = Phrases(docs, min_count = 20)
for index in range(len(docs)):
  for token in bigram[docs[index]]:
    if '_' in token:
      docs[index].append(token) #add bigram to document

"""Now, our docs should also have bigrams. """

str(docs[0])

"""At this point, we have several words that should be common across all documents, as well as words that only appear once or twice and never again. We remove those using the `gensim.corpora` `Dictionary` package."""

from gensim.corpora import Dictionary

dictionary = Dictionary(docs)

dictionary.filter_extremes(no_below=10, no_above=0.5)

"""And then lastly, let's check the overall features of our data again."""

print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))

"""### 2.2.2 Finding the Ideal Topic Number

Now that we've organized our data, the next thing we need to do is make sure we choose the right number of topics.

First, we assign a few important variables that will be used for all our models (i.e. corpus and id2word)
"""

corpus = [dictionary.doc2bow(doc) for doc in docs] #our collection of texts
workers = 20,
passes = 10,
iterations = 50,
random_state = 1
temp = dictionary[0] # this makes sure our dictionary is implemented
id2word = dictionary.id2token

def compute_coherence(k):
  lda_model = gensim.models.LdaMulticore(corpus = corpus, 
                                         id2word = id2word, 
                                         num_topics = k, 
                                         workers = 20,
                                         passes = 10,
                                         iterations = 50,
                                         random_state = 1)
  coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts = docs, dictionary = dictionary, coherence='c_v')
  return coherence_model_lda.get_coherence(), k

"""Given what we've learned from K-means clustering and the optimal 12 clusters, we can expect that there should be more than 5 topics but likely no more than 20. So we can use our function to search for topic count with the highest coherence."""

coherence_scores_df = pd.DataFrame(columns=['coherence_score','num_topics'])

for num in range(5,20):
  coherence_score, num_topics = compute_coherence(num)
  temp_df = pd.DataFrame({'coherence_score': [coherence_score], 'num_topics': [num_topics]})
  coherence_scores_df = coherence_scores_df.append(temp_df)
#Let's See the Scores
coherence_scores_df

coherence_scores_df.plot(x='num_topics', y='coherence_score',kind='line')

"""It seems like generally, maximum coherence is achieved at 12 topics as well, so we will also create 12 topics for our final model.

### 2.2.3 A 12-Topic LDA Model

With the data prepared, we can try running an LDA model to see if we can come up with salient topics. We've already set up important variables, so the only thing left is to run the model for 12 topics.
"""

num_topics = 12 # We make this a separate code block in case results somehow change (they shouldn't)

lda_model = gensim.models.LdaMulticore(corpus = corpus, 
                                         id2word = id2word, 
                                         num_topics = num_topics, 
                                         workers = 20,
                                         passes = 10,
                                         iterations = 50,
                                         random_state = 1)

"""With the model run, we look at the topics put out by the model."""

from pprint import pprint
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

"""We can also calculate the coherence of the model."""

cm = gensim.models.CoherenceModel(model=lda_model, texts = docs, dictionary = dictionary, coherence = 'c_v')
coherence = (cm.get_coherence(), num_topics)
coherence

"""And we can visualize the topics with pyLDAvis."""

import pyLDAvis.gensim
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(num_topics))
# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself
if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
    with open(LDAvis_data_filepath, 'wb') as f:
        pickle.dump(LDAvis_prepared, f)
# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)
pyLDAvis.save_html(LDAvis_prepared, 'ldavis_prepared_'+ str(num_topics) +'.html')
LDAvis_prepared

"""Many of the topics are actually very similar to the topics generated by K-Means. For example, Topic 2 is related to academia, and Topic 10 is mainly miscellaneous Javascript and login credentials. 

**Note:** to see this vizualization, you'll need to run the ipynb version of this notebook yourself. We haven't quite figured out how to get this visualization to render in the HTML export.

### 2.2.4 Segmentation Using LDA

Now that we have a model trained, we can use it to generate topic probability distributions for each website. For example, we can generate the probability of topics for Cornell University, shown below.
"""

test_doc = corpus[0]
vector = lda_model[test_doc]
print(docs[0])
print(vector)

"""Overall, the topics separated out by our LDA model are interesting but not as intuitive as in our K-Means clustering. As our objective is to make a tool that can be easily used by content-creators, K-Means clustering is a better bet."""